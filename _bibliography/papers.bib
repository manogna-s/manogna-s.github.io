@misc{sreenivas2023pstarc,
      abbr={arXiv Preprint},
      title={pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation},
      author={Manogna Sreenivas and Goirik Chakrabarty and Soma Biswas},
      year={2023},
      eprint={2309.00846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract= "Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo-source samples. The test samples are strategically aligned with these pseudo-source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC’s effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework.",
      html={https://arxiv.org/abs/2309.00846},
      pdf={https://arxiv.org/pdf/2309.00846.pdf}
}


@InProceedings{dss,
abbr={ICCVW},
author="Chakrabarty, Goirik
and Sreenivas, Manogna
and Biswas, Soma",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="A Simple Signal for Domain Shift",
booktitle="ICCV Workshops",
year="2023",
publisher="Springer Nature Switzerland",
abstract="Test time domain adaptation has come to the forefront as a challenging scenario in recent times. Although single domain test-time adaptation has been well studied and shown impressive performance, this can be limiting when the model is deployed in a dynamic test environment. We explore this continual domain test time adaptation problem here. Specifically, we question if we can translate the effectiveness of single domain adaptation methods to continuous test-time adaptation scenario. We take a step towards bridging the gap between these two settings by proposing a domain shift detection mechanism and hence allowing us to employ the current test-time adaptation methods even in a continual setting. We propose to use the given source domain trained model to continually measure the similarity between the feature representations of the consecutive batches. A domain shift is detected when this measure crosses a certain threshold, which we use as a trigger to reset the model back to source and continue test-time adaptation. We demonstrate the effectiveness of our method by performing experiments across datasets, batch sizes and different single domain test-time adaptation baselines.",
address="Cham",
pages="262--277",
isbn="978-3-031-25072-9",
selected={true}
}



@InProceedings{10.1007/978-3-031-25072-9_17,
abbr={CVPRW},
author="Sreenivas, Manogna
and Biswas, Soma",
title="Similar Class Style Augmentation for Efficient Cross-Domain Few-Shot Learning",
booktitle="CVPR Workshops",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
abstract="Cross-Domain Few-Shot Learning (CD-FSL) aims to recognize new classes from unseen domains, given limited training samples. Majority of the state-of-the-art approaches for this task introduce new task-specific additional parameters for adapting to the novel task, which involves changing the trained model architecture, in addition to increasing the number of model parameters. The first contribution of this work is to revisit the existing approaches like modifying the Batch Normalization affine parameters and the scale hyperparameter in cosine similarity based softmax loss for adapting the trained model to the new tasks, without changing the model architecture. Secondly, to aid the model learning with few examples per class, we propose to augment the data of each class with the styles of the semantically similar classes. Extensive evaluation on the challenging Meta-Dataset shows that this simple framework is very effective for the CD-FSL task. We also show that the Similar-class Style Augmentation module can be seamlessly integrated with existing approaches to further improve their performance, thus establishing the state-of-the-art in this challenging area.",
isbn="978-3-031-25072-9",
html={https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.html},
pdf={https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.pdf},

selected={true}
}

@misc{chakrabarty2023sata,
      abbr={arXiv Preprint},
      title={SATA: Source Anchoring and Target Alignment Network for Continual Test Time Adaptation},
      author={Goirik Chakrabarty and Manogna Sreenivas and Soma Biswas},
      year={2023},
      eprint={2304.10113},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract = "Adapting a trained model to perform satisfactorily on continually changing testing domains/environments is an important and challenging task. In this work, we propose a novel framework, SATA, which aims to satisfy the following characteristics required for online adaptation: 1) can work seamlessly with different (preferably small) batch sizes to reduce latency; 2) should continue to work well for the source domain; 3) should have minimal tunable hyper-parameters and storage requirements. Given a pre-trained network trained on source domain data, the proposed SATA framework modifies the batch-norm affine parameters using source anchoring based self-distillation. This ensures that the model incorporates the knowledge of the newly encountered domains, without catastrophically forgetting about the previously seen ones. We also propose a source-prototype driven contrastive alignment to ensure natural grouping of the target samples, while maintaining the already learnt semantic information. Extensive evaluation on three benchmark datasets under challenging settings justify the effectiveness of SATA for real-world applications.",
      html={https://arxiv.org/abs/2304.10113},
      pdf={https://arxiv.org/pdf/2304.10113.pdf}
}

@InProceedings{10.1007/978-3-031-25072-9_17,
abbr={ECCVW},
author="Sreenivas, Manogna
and Takamuku, Sawa
and Biswas, Soma
and Chepuri, Aditya
and Vengatesan, Balasubramanian
and Natori, Naotake",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="Improved Cross-Dataset Facial Expression Recognition by Handling Data Imbalance and Feature Confusion",
booktitle="ECCV Workshops",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
abstract="Facial Expression Recognition (FER) models trained on one dataset (source) usually do not perform well on a different dataset (target) due to the implicit domain shift between different datasets. In addition, FER data is naturally highly imbalanced, with a majority of the samples belonging to few expressions like neutral, happy and relatively fewer samples coming from expressions like disgust, fear, etc., which makes the FER task even more challenging. This class imbalance of the source and target data (which may be different), along with other factors like similarity of few expressions, etc., can result in unsatisfactory target classification performance due to confusion between the different classes. In this work, we propose an integrated module, termed DIFC, which can not only handle the source Data Imbalance, but also the Feature Confusion of the target data for improved classification of the target expressions.We integrate this DIFC module with an existing Unsupervised Domain Adaptation (UDA) approach to handle the domain shift and show that the proposed simple yet effective module can result in significant performance improvement on four benchmark datasets for Cross-Dataset FER (CD-FER) task. We also show that the proposed module works across different architectures and can be used with other UDA baselines to further boost their performance.",
isbn="978-3-031-25072-9",
html={https://link.springer.com/chapter/10.1007/978-3-031-25072-9_17},
pdf={https://github.com/manogna-s/da-fer/blob/main/DIFC_WCPA_ECCVW'22.pdf},
selected={true}
}


@InProceedings{dss,
abbr={ICLRW},
author="Chakrabarty, Goirik
and Sreenivas, Manogna
and Biswas, Soma",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="Domain Shift Signal for Low Resource Continuous Test-Time Adaptation",
booktitle="ICLR Workshops",
abstract="Test time domain adaptation has come to the forefront as a challenging scenario in recent times. Although single domain test-time adaptation has been well studied and shown impressive performance, this can be limiting when the model is deployed in a dynamic test environment. We explore this continual domain test time adaptation problem here. Specifically, we question if we can translate the effectiveness of single domain adaptation methods to continuous test-time adaptation scenario. We propose to use the given source domain trained model to continually measure the similarity between the feature representations of the consecutive batches. A domain shift is detected when this measure falls below a certain threshold, which we use as a trigger to reset the model back to source and continue test-time adaptation. We demonstrate the effectiveness of our method by performing experiments across datasets, batch sizes and different single domain test-time adaptation baselines. This can have a significant impact in a variety of applications, from healthcare and agriculture to transportation and finance. As a result, this research has the potential to greatly benefit developing countries by providing new tools and techniques for building more effective and efficient machine learning systems.",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
isbn="978-3-031-25072-9",
selected={false},
pdf={https://pml4dc.github.io/iclr2023/pdf/PML4DC_ICLR2023_46.pdf}
}

@InProceedings{jumpstyle,
abbr={ICLRW},
author="Singh, Aakash
and Sreenivas, Manogna
and Biswas, Soma",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="JumpStyle: A Framework for Data-Efficient Online Adaptation",
booktitle="ICLR Workshops",
abstract="Research in deep learning is restrictive in developing countries due to a lack of computational resources, quality training data, and expert knowledge, which negatively impacts the performance of deep networks. Moreover, these models are prone to suffer from distribution shift during testing. To address these challenges, this paper presents a novel approach for fine-tuning deep networks in a Domain Generalization setting. The proposed framework, JumpStyle, comprises two key components: (1) an innovative initialization technique that jumpstarts the adaptation process, and (2) the use of style-aware augmentation with pseudo-labeling, in conjunction with a simple and effective test-time adaptation baseline named Tent. Importantly, JumpStyle only requires access to a pre-trained model and is not limited by the training method. The effectiveness of this approach is extensively evaluated through experiments.",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
isbn="978-3-031-25072-9",
selected={false},
pdf={https://pml4dc.github.io/iclr2023/pdf/PML4DC_ICLR2023_47.pdf}
}

