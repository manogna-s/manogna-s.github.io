@InProceedings{sreenivas2024rosita,
      abbr={arXiV},
      title={Effectiveness of Vision Language Models for Open-world Single Image Test Time Adaptation},
      author={Manogna Sreenivas and Soma Biswas},
      year={2024},
      booktitle="PrePrint. Under Review",
      eprint={2406.00481},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract = "We propose a novel framework to address the real-world challenging task of Single Image Test Time Adaptation in an open and dynamic environment. We leverage large scale Vision Language Models like CLIP to enable real time adaptation on a per-image basis without access to source data or ground truth labels. Since the deployed model can also encounter unseen classes in an open world, we first employ a simple and effective Out of Distribution (OOD) detection module to distinguish between weak and strong OOD samples. We propose a novel contrastive learning based objective to enhance the discriminability between weak and strong OOD samples by utilizing  small, dynamically updated feature banks. Finally, we also employ a classification objective for adapting the model using the reliable weak OOD samples. The proposed framework ROSITA combines these components, enabling continuous online adaptation of Vision Language Models on a single image basis. Extensive experimentation on diverse domain adaptation benchmarks validates the effectiveness of the proposed framework.",
      website={https://manogna-s.github.io/rosita/},
      arxiv={2406.00481},
      code = {https://github.com/manogna-s/ROSITA},
      selected={true}
}

@InProceedings{sreenivas2023pstarc,
      abbr={WACV},
      title={pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation},
      author={Manogna Sreenivas and Goirik Chakrabarty and Soma Biswas},
      year={2024},
      booktitle="Winter Conference on Applications of Computer Vision",
      eprint={2309.00846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract= "Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo-source samples. The test samples are strategically aligned with these pseudo-source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC’s effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework.",
      website={https://manogna-s.github.io/pstarc/},
      arxiv={2309.00846},
      code={https://github.com/manogna-s/pSTarC},
      selected={true}
}

@InProceedings{phish-net,
      abbr={WACV},
      title={PhISH-Net: Physics Inspired System for High Resolution Underwater Image Enhancement},
      author={Aditya Chandrasekar and Manogna Sreenivas and Soma Biswas},
      year={2024},
      booktitle="Winter Conference on Applications of Computer Vision",
      eprint={2309.00846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract= "Underwater imaging presents numerous challenges due to refraction, light absorption, and scattering, resulting in color degradation, low contrast, and blurriness. Enhancing underwater images is crucial for high-level computer vision tasks, but existing methods either neglect the physics-based image formation process or require expensive computations. In this paper, we propose an effective framework that combines a physics-based Underwater Image Formation Model (UIFM) with a deep image enhancement approach based on the retinex model. Firstly, we remove backscatter by estimating attenuation coefficients using depth information. Then, we employ a retinex model-based deep image enhancement module to enhance the images. To ensure adherence to the UIFM, we introduce a novel Wideband Attenuation prior. The proposed PhISH-Net framework achieves real-time processing of high-resolution underwater images using a lightweight neural network and a bilateral-grid-based upsampler. Extensive experiments on two underwater image datasets demonstrate the superior performance of our method compared to state-of-the-art techniques. Additionally, qualitative evaluation on a cross-dataset scenario confirms its generalization capability. Our contributions lie in combining the physics-based UIFM with deep image enhancement methods, introducing the wideband attenuation prior, and achieving superior performance and efficiency.",
      pdf={https://openaccess.thecvf.com/content/WACV2024/papers/Chandrasekar_PhISH-Net_Physics_Inspired_System_for_High_Resolution_Underwater_Image_Enhancement_WACV_2024_paper.pdf},
      selected={false}
}


@InProceedings{chakrabarty2023sata,
      abbr={TMLR},
      title={SANTA: Source Anchoring Network and Target Alignment for Continual Test Time Adaptation},
      author={Goirik Chakrabarty and Manogna Sreenivas and Soma Biswas},
      year={2023},
      booktitle="Transactions in Machine Learning Research",
      eprint={2304.10113},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      abstract = "Adapting a trained model to perform satisfactorily on continually changing test environ- ments is an important and challenging task. In this work, we propose a novel framework, SANTA, which aims to satisfy the following characteristics required for online adaptation: 1) can work effectively for different (even small) batch sizes; 2) should continue to work well on the source domain; 3) should have minimal tunable hyperparameters and storage requirements. Given a pre-trained network trained on source domain data, the proposed framework modifies the affine parameters of the batch normalization layers using source anchoring based self-distillation. This ensures that the model incorporates knowledge from the newly encountered domains, without catastrophically forgetting the previously seen do- mains. We also propose a source-prototype driven contrastive alignment to ensure natural grouping of the target samples, while maintaining the already learnt semantic information. Extensive evaluation on three benchmark datasets under challenging settings justify the effectiveness of SANTA for real-world applications.",
      html={https://openreview.net/forum?id=V7guVYzvE4},
      code = {https://github.com/goirik-chakrabarty/SANTA},
      pdf={https://openreview.net/pdf?id=V7guVYzvE4},
      selected={true}
}

@InProceedings{dss,
abbr={ICCVW},
author="Chakrabarty, Goirik
and Sreenivas, Manogna
and Biswas, Soma",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="A Simple Signal for Domain Shift",
booktitle="ICCV Workshops",
year="2023",
publisher="Springer Nature Switzerland",
abstract="Test time domain adaptation has come to the forefront as a challenging scenario in recent times. Although single domain test-time adaptation has been well studied and shown impressive performance, this can be limiting when the model is deployed in a dynamic test environment. We explore this continual domain test time adaptation problem here. Specifically, we question if we can translate the effectiveness of single domain adaptation methods to continuous test-time adaptation scenario. We take a step towards bridging the gap between these two settings by proposing a domain shift detection mechanism and hence allowing us to employ the current test-time adaptation methods even in a continual setting. We propose to use the given source domain trained model to continually measure the similarity between the feature representations of the consecutive batches. A domain shift is detected when this measure crosses a certain threshold, which we use as a trigger to reset the model back to source and continue test-time adaptation. We demonstrate the effectiveness of our method by performing experiments across datasets, batch sizes and different single domain test-time adaptation baselines.",
address="Cham",
pages="262--277",
isbn="978-3-031-25072-9",
pdf={https://openaccess.thecvf.com/content/ICCV2023W/VCL/papers/Chakrabarty_A_Simple_Signal_for_Domain_Shift_ICCVW_2023_paper.pdf},
selected={false}
}



@InProceedings{10.1007/978-3-031-25072-9_17,
abbr={CVPRW},
author="Sreenivas, Manogna
and Biswas, Soma",
title="Similar Class Style Augmentation for Efficient Cross-Domain Few-Shot Learning",
booktitle="CVPR Workshops",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
abstract="Cross-Domain Few-Shot Learning (CD-FSL) aims to recognize new classes from unseen domains, given limited training samples. Majority of the state-of-the-art approaches for this task introduce new task-specific additional parameters for adapting to the novel task, which involves changing the trained model architecture, in addition to increasing the number of model parameters. The first contribution of this work is to revisit the existing approaches like modifying the Batch Normalization affine parameters and the scale hyperparameter in cosine similarity based softmax loss for adapting the trained model to the new tasks, without changing the model architecture. Secondly, to aid the model learning with few examples per class, we propose to augment the data of each class with the styles of the semantically similar classes. Extensive evaluation on the challenging Meta-Dataset shows that this simple framework is very effective for the CD-FSL task. We also show that the Similar-class Style Augmentation module can be seamlessly integrated with existing approaches to further improve their performance, thus establishing the state-of-the-art in this challenging area.",
isbn="978-3-031-25072-9",
pdf={https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.pdf},
selected={false}
}


@InProceedings{10.1007/978-3-031-25072-9_17,
abbr={ECCVW},
author="Sreenivas, Manogna
and Takamuku, Sawa
and Biswas, Soma
and Chepuri, Aditya
and Vengatesan, Balasubramanian
and Natori, Naotake",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="Improved Cross-Dataset Facial Expression Recognition by Handling Data Imbalance and Feature Confusion",
booktitle="ECCV Workshops",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
abstract="Facial Expression Recognition (FER) models trained on one dataset (source) usually do not perform well on a different dataset (target) due to the implicit domain shift between different datasets. In addition, FER data is naturally highly imbalanced, with a majority of the samples belonging to few expressions like neutral, happy and relatively fewer samples coming from expressions like disgust, fear, etc., which makes the FER task even more challenging. This class imbalance of the source and target data (which may be different), along with other factors like similarity of few expressions, etc., can result in unsatisfactory target classification performance due to confusion between the different classes. In this work, we propose an integrated module, termed DIFC, which can not only handle the source Data Imbalance, but also the Feature Confusion of the target data for improved classification of the target expressions.We integrate this DIFC module with an existing Unsupervised Domain Adaptation (UDA) approach to handle the domain shift and show that the proposed simple yet effective module can result in significant performance improvement on four benchmark datasets for Cross-Dataset FER (CD-FER) task. We also show that the proposed module works across different architectures and can be used with other UDA baselines to further boost their performance.",
isbn="978-3-031-25072-9",
html={https://link.springer.com/chapter/10.1007/978-3-031-25072-9_17},
pdf={https://github.com/manogna-s/da-fer/blob/main/DIFC_WCPA_ECCVW'22.pdf},
selected={false}
}


@InProceedings{jumpstyle,
abbr={ICLRW},
author="Singh, Aakash
and Sreenivas, Manogna
and Biswas, Soma",
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="JumpStyle: A Framework for Data-Efficient Online Adaptation",
booktitle="ICLR Workshops",
abstract="Research in deep learning is restrictive in developing countries due to a lack of computational resources, quality training data, and expert knowledge, which negatively impacts the performance of deep networks. Moreover, these models are prone to suffer from distribution shift during testing. To address these challenges, this paper presents a novel approach for fine-tuning deep networks in a Domain Generalization setting. The proposed framework, JumpStyle, comprises two key components: (1) an innovative initialization technique that jumpstarts the adaptation process, and (2) the use of style-aware augmentation with pseudo-labeling, in conjunction with a simple and effective test-time adaptation baseline named Tent. Importantly, JumpStyle only requires access to a pre-trained model and is not limited by the training method. The effectiveness of this approach is extensively evaluated through experiments.",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="262--277",
isbn="978-3-031-25072-9",
selected={false},
pdf={https://pml4dc.github.io/iclr2023/pdf/PML4DC_ICLR2023_47.pdf}
}

